{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploring DM Tools.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlE5lbDuY4Oplonlm/PbEI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jom-2021/test_cypher/blob/main/Exploring_DM_Tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFi8ILvVxHtG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring DM Tools from neo4j Database"
      ],
      "metadata": {
        "id": "aZJOMCegxzkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download neo4j-community-4.4.9-alpha09mr02-unix\n",
        "!curl https://dist.neo4j.org/neo4j-community-4.4.9-unix.tar.gz -o neo4j.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDx--A7Fx3je",
        "outputId": "9cdca7fc-27bb-4d58-ee0f-4698c016b6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  125M  100  125M    0     0   151M      0 --:--:-- --:--:-- --:--:--  151M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decompress and rename\n",
        "!tar -xf neo4j.tar.gz  # or --strip-components=1\n",
        "!mv neo4j-community-4.4.9 nj\n"
      ],
      "metadata": {
        "id": "6IpmXUEhefJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# disable password, and start server\n",
        "!sed -i '/#dbms.security.auth_enabled/s/^#//g' nj/conf/neo4j.conf\n",
        "!nj/bin/neo4j start"
      ],
      "metadata": {
        "id": "OACEA5wlelqr",
        "outputId": "38b9b9ef-e951-4146-8ce8-b97538897c95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories in use:\n",
            "home:         /content/nj\n",
            "config:       /content/nj/conf\n",
            "logs:         /content/nj/logs\n",
            "plugins:      /content/nj/plugins\n",
            "import:       /content/nj/import\n",
            "data:         /content/nj/data\n",
            "certificates: /content/nj/certificates\n",
            "licenses:     /content/nj/licenses\n",
            "run:          /content/nj/run\n",
            "Starting Neo4j.\n",
            "Started neo4j (pid:151). It is available at http://localhost:7474\n",
            "There may be a short delay until the server is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download and config\n",
        "!wget -P nj/plugins https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/4.4.0.8/apoc-4.4.0.8-all.jar"
      ],
      "metadata": {
        "id": "KV1nwD3mj5o2",
        "outputId": "02f63438-13fe-43fa-fd46-49c67f01b09b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-05 08:21:13--  https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/4.4.0.8/apoc-4.4.0.8-all.jar\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/52509220/bff69254-54c1-49a1-9244-f89e304784b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220805%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220805T082008Z&X-Amz-Expires=300&X-Amz-Signature=c22be16504648ea7875f96cf6b3397e68bfd5611091cee101b5b8063e6bf8fee&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=52509220&response-content-disposition=attachment%3B%20filename%3Dapoc-4.4.0.8-all.jar&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-08-05 08:21:13--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/52509220/bff69254-54c1-49a1-9244-f89e304784b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220805%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220805T082008Z&X-Amz-Expires=300&X-Amz-Signature=c22be16504648ea7875f96cf6b3397e68bfd5611091cee101b5b8063e6bf8fee&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=52509220&response-content-disposition=attachment%3B%20filename%3Dapoc-4.4.0.8-all.jar&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21642527 (21M) [application/octet-stream]\n",
            "Saving to: ‘nj/plugins/apoc-4.4.0.8-all.jar’\n",
            "\n",
            "apoc-4.4.0.8-all.ja 100%[===================>]  20.64M  32.6MB/s    in 0.6s    \n",
            "\n",
            "2022-08-05 08:21:14 (32.6 MB/s) - ‘nj/plugins/apoc-4.4.0.8-all.jar’ saved [21642527/21642527]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/dbms.security.procedures.unrestricted/s/^#//g' nj/conf/neo4j.conf\n",
        "!sed -i 's/my.extensions.example,my.procedures/apoc/' nj/conf/neo4j.conf"
      ],
      "metadata": {
        "id": "I_AJ3MC_j5xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nj/bin/neo4j restart"
      ],
      "metadata": {
        "id": "FNc80SxYj51m",
        "outputId": "75ebd3b2-120b-4711-8b49-42b58d8eb0fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping Neo4j....... stopped.\n",
            "Directories in use:\n",
            "home:         /content/nj\n",
            "config:       /content/nj/conf\n",
            "logs:         /content/nj/logs\n",
            "plugins:      /content/nj/plugins\n",
            "import:       /content/nj/import\n",
            "data:         /content/nj/data\n",
            "certificates: /content/nj/certificates\n",
            "licenses:     /content/nj/licenses\n",
            "run:          /content/nj/run\n",
            "Starting Neo4j.\n",
            "Started neo4j (pid:329). It is available at http://localhost:7474\n",
            "There may be a short delay until the server is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cypher\n",
        "%config CypherMagic.feedback=False"
      ],
      "metadata": {
        "id": "MQ-VyagyiX5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install icypher"
      ],
      "metadata": {
        "id": "DWUHQZHMq0Vk",
        "outputId": "8f64eecf-f100-4e10-a3ab-c1c0a1025e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting icypher\n",
            "  Downloading icypher-0.1.6.1-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: ipython>=4.0 in /usr/local/lib/python3.7/dist-packages (from icypher) (5.5.0)\n",
            "Collecting py2neo>=3.0\n",
            "  Downloading py2neo-2021.2.3-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 15.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0->icypher) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0->icypher) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0->icypher) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0->icypher) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0->icypher) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0->icypher) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0->icypher) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0->icypher) (2.6.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0->icypher) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0->icypher) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from py2neo>=3.0->icypher) (1.24.3)\n",
            "Collecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting pansi>=2020.7.3\n",
            "  Downloading pansi-2020.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from py2neo>=3.0->icypher) (2022.6.15)\n",
            "Collecting interchange~=2021.0.4\n",
            "  Downloading interchange-2021.0.4-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from py2neo>=3.0->icypher) (21.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from interchange~=2021.0.4->py2neo>=3.0->icypher) (2022.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->py2neo>=3.0->icypher) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=4.0->icypher) (0.7.0)\n",
            "Installing collected packages: pansi, monotonic, interchange, py2neo, icypher\n",
            "Successfully installed icypher-0.1.6.1 interchange-2021.0.4 monotonic-1.6 pansi-2020.7.3 py2neo-2021.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext icypher"
      ],
      "metadata": {
        "id": "DU7H00VNq5Tw",
        "outputId": "ff2d11f8-057b-412e-b64c-49fa1e50423b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "require(['notebook/js/codecell'], function(codecell) {\n",
              "  codecell.CodeCell.options_default.highlight_modes['magic_application/x-cypher-query'] = {'reg':[/^%%cypher/]};\n",
              "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
              "      Jupyter.notebook.get_cells().map(function(cell){\n",
              "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
              "  });\n",
              "});\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cypher http://localhost:7474/db/data"
      ],
      "metadata": {
        "id": "F4ToZx_Vq__-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cypher match (p) return p"
      ],
      "metadata": {
        "id": "7UhBZ1JLtEaO",
        "outputId": "c146492b-b33d-478c-b5a1-e7d5bf474f84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test \n",
        "\n",
        "%%cypher\n",
        "CREATE (n{name:'cypher'})-[r:LIKES]->({name:'icecream'}) return n.name, r"
      ],
      "metadata": {
        "id": "2C2OG0q8rAIp",
        "outputId": "cc794cd5-e717-4da4-9513-245aebb3e6bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'n.name': 'cypher', 'r': LIKES(Node(name='cypher'), Node(name='icecream'))}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "\n",
        "%%cypher\n",
        "MATCH (p) return p\n",
        "//graph_result"
      ],
      "metadata": {
        "id": "B6O3KGnvrAL_",
        "outputId": "c4f42a8c-285b-40bc-e7ac-8ab5307f6a7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'p': Node(name='cypher')},\n",
              " {'p': Node(name='icecream')},\n",
              " {'p': Node('is_retired', 'tool', description='', name='Apache apex')},\n",
              " {'p': Node('origin', name='databricks')},\n",
              " {'p': Node('origin', description='', name='facebook')},\n",
              " {'p': Node('origin', description='', name='apache')},\n",
              " {'p': Node('origin', description='', name='cloudera')},\n",
              " {'p': Node('origin', name='linkedIn')},\n",
              " {'p': Node('origin', name='AirBnB')},\n",
              " {'p': Node('origin', name='Lucene')},\n",
              " {'p': Node('tool', description='', name='Aura DB')},\n",
              " {'p': Node('tool', description='', name='Aura DS')},\n",
              " {'p': Node('tool', description='', name='neo4j')},\n",
              " {'p': Node('tool', name='cypher')},\n",
              " {'p': Node('tool', name='Apache nessie')},\n",
              " {'p': Node('tool', description='Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.', name='Apache beam', url='https://beam.apache.org/')},\n",
              " {'p': Node('tool', description='Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.', name='Apache iceberg')},\n",
              " {'p': Node('tool', description='Apache kafka is a middleware for the reliable transmission of events and for stream processing (kSQL). There exist a lot of connectors to connect with different data sources. Professional support (and more connectors and a managed service and a schema registry) is available from confluent.', name='Apache kafka')},\n",
              " {'p': Node('tool', description='Framework for the transmission of data on columnar format. It provides a protocol for the transmission without the need to convert it into rows (as it would be necessary when using odbc). It supports distributed processing.', name='Apache arrow flight')},\n",
              " {'p': Node('tool', description='Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.', name='Apache flink')},\n",
              " {'p': Node('tool', description='Apache Samza is a scalable data processing engine that allows you to process and analyze your data in real-time. Here is a summary of Samza’s features that simplify building your applications. Unified API: Use a simple API to describe your application-logic in a manner independent of your data-source. The same API can process both batch and streaming data.', name='Apache samza', url='https://samza.apache.org/')},\n",
              " {'p': Node('tool', description='Quine is built around a novel design choice: to represent both the data model and the computational model using a graph. In fact, the same graph is used for both models. Using the same model for both the data model and the computational model helps provide remarkable new capabilities from the Quine system.', name='Quine', url='https://docs.quine.io/core-concepts/core-concepts.html')},\n",
              " {'p': Node('tool', description='Snowflake is a fully managed SaaS (software as a service) that provides a single platform for data warehousing, data lakes, data engineering, data science, data application development, and secure sharing and consumption of real-time / shared data. Snowflake features out-of-the-box features like separation of storage and compute, on-the-fly scalable compute, data sharing, data cloning, and third-party tools support in order to handle the demanding needs of growing enterprises.', name='snowflake')},\n",
              " {'p': Node('tool', name='Elastic search')},\n",
              " {'p': Node('tool', name='dremio')},\n",
              " {'p': Node('tool', description='Yarn is a package manager that doubles down as project manager. Whether you work on one-shot projects or large monorepos, as a hobbyist or an enterprise user, we have got you covered.', name='yarn', url='https://yarnpkg.com/')},\n",
              " {'p': Node('tool', description='', name='jq-assistant')},\n",
              " {'p': Node('tool', description='', name='hadoop')},\n",
              " {'p': Node('tool', description='Impala is a massively parallel processing engine that is an open source engine. It requires the database to be stored in clusters of computers that are running Apache Hadoop. It is a SQL engine, launched by Cloudera in 2012.', name='Apache impala', url='https://impala.apache.org/index.html')},\n",
              " {'p': Node('tool', description='Apache Hive is basically an open source data warehouse system which can handle, query, or analyze large datasets and process structured/non-structured data in Hadoop. Apache Hive is built on Hadoop big data platform.', name='hive')},\n",
              " {'p': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'p': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'p': Node('tool', description='', name='prestoSQL')},\n",
              " {'p': Node('tool', description='Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.', do_not_use_for='analytics', name='Apache cassandra')},\n",
              " {'p': Node('tool', description='Apache Pig[1] is a high-level platform for creating programs that run on Apache Hadoop. The language for this platform is called Pig Latin.[1] Pig can execute its Hadoop jobs in MapReduce, Apache Tez, or Apache Spark.[2] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for relational database management systems. Pig Latin can be extended using user-defined functions (UDFs) which the user can write in Java, Python, JavaScript, Ruby or Groovy[3] and then call directly from the language.', name='Apache pig', url='https://pig.apache.org/')},\n",
              " {'p': Node('tool', description='An intelligent metastore for Apache Iceberg that uniquely provides users a Git-like experience for data and automatically optimizes data to ensure high performance analytics.', name='dremio arctic')},\n",
              " {'p': Node('tool', description='A SQL engine for open platforms that provides data warehouse-level performance and capabilities on the data lake, and a self-service experience that makes data consumable and collaborative.', name='dremio sonar')},\n",
              " {'p': Node('tool', description='', name='Power BI')},\n",
              " {'p': Node('tool', description='', name='Tableau')},\n",
              " {'p': Node('tool', description='Developer-friendly interface for connecting to data systems with Arrow Flight endpoints (like odbc/jdbc).', name='Apache arrow flight sql')},\n",
              " {'p': Node('tool', description='', name='prestoDB')},\n",
              " {'p': Node('tool', description='Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto was designed and written from the ground up for interactive analytics and approaches the speed of commercial data warehouses while scaling to the size of organizations like Facebook.', name='presto', url='https://prestodb.io/')},\n",
              " {'p': Node('tool', description='Apache Hudi (pronounced “hoodie”) is the next generation streaming data lake platform. Apache Hudi brings core warehouse and database functionality directly to a data lake. Hudi provides tables, transactions, efficient upserts/deletes, advanced indexes, streaming ingestion services, data clustering/compaction optimizations, and concurrency all while keeping your data in open source file formats.', name='Apache hudi', url='https://hudi.apache.org/docs/overview')},\n",
              " {'p': Node('tool', description='Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.', name='delta lake', url='https://delta.io')},\n",
              " {'p': Node('tool', description='Apache Storm is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Apache Storm is simple, can be used with any programming language, and is a lot of fun to use! Apache Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Apache Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate. Apache Storm integrates with the queueing and database technologies you already use. An Apache Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed.', name='Apache storm', url='https://storm.apache.org/')},\n",
              " {'p': Node('tool', description='Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API. When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.', name='Apache spark SQL', url='https://spark.apache.org/docs/latest/sql-programming-guide.html')},\n",
              " {'p': Node('tool', description='Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.', name='Apache spark Structured Streaming', url='https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html')},\n",
              " {'p': Node('tool', description='GraphX is a new component in Spark for graphs and graph-parallel computation. At a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge. To support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.', name='Apache spark GraphX', url='https://spark.apache.org/docs/latest/graphx-programming-guide.html#overview')},\n",
              " {'p': Node('tool', description='Drill is a distributed query engine that doesnt require schemas. The data structure is infered and queries can span multiple sources. Sources are processed in-situ. The service can run on a single laptop or can scale via hadoop. Yarn is leveraged for the ressource management. It provides a standard SQL query language and supports common BI tools like tableau or PowerBI.', name='Apache drill', url='https://drill.apache.org/')},\n",
              " {'p': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'p': Node('tool', description='Teradata Vantage is a platform for data analytics. It is based on a RDBMS. It allows installation in multiple public and private clouds.', name='teradata Vantage', url='https://www.teradata.de/Vantage')},\n",
              " {'p': Node('tool', description='In a class by itself, only Apache HAWQ combines exceptional MPP-based analytics performance, robust ANSI SQL compliance, Hadoop ecosystem integration and manageability, and flexible data-store format support. All natively in Apache Hadoop. No connectors required.', name='Apache hawq', url='https://hawq.apache.org/')},\n",
              " {'p': Node('tool', description=\"Apache HBase™ is the Hadoop database, a distributed, scalable, big data store. Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.\", name='Apache hbase', url='https://hbase.apache.org/')},\n",
              " {'p': Node('tool', description='Apache nemo is a framework to optimise the scheduling and communication of distributed data processing. It places itself between Applications (e.g. on Apache beam) and the distributed parallel processing engines (such like Apache spark). https://www.usenix.org/system/files/atc19-yang-youngseok.pdf and https://www.usenix.org/conference/atc19/presentation/yang-youngseok', name='Apache nemo', url='https://nemo.apache.org/docs/home/')},\n",
              " {'p': Node('tool', description='Airflow is a platform created by the community to programmatically author, schedule and monitor workflows. Apache Airflow is an open-source WMS designed for authoring, scheduling, and monitoring workflows as DAGs (directed acyclic graphs). Workflows are written in Python, which allows flexible interaction with third-party APIs, databases, and data systems. Data pipelines in Airflow are built by defining a set of tasks to extract, transform, load, analyze or store the data. Airflow is a workflow scheduler designed to help with scheduling complex workflows and provide them an easy way to maintain them. It is a great product for data engineering if you use it with the purpose it was designed for – to orchestrate work executed on external systems such as Spark, Hadoop, Druid, cloud services, etc. For example, if your task is to load data in PostgreSQL, make some aggregations using Spark, and store the data on your Hadoop cluster (like in Figure 2.), then Airflow is the best choice since it can tie many external systems together. Airflow was not designed to execute any workflows directly inside but to schedule them and keep the execution within external systems.', name='Apache airflow', url='https://airflow.apache.org/')},\n",
              " {'p': Node('tool', description='Instant GraphQL on all your data. Run Hasura, locally or in the cloud, and connect it to your new or existing databases to instantly get a production grade GraphQL API.', name='hasura', url='https://hasura.io/')},\n",
              " {'p': Node('tool', description='Solr is a standalone enterprise search server with a REST-like API. You put documents in it (called \"indexing\") via JSON, XML, CSV or binary over HTTP. You query it via HTTP GET and receive JSON, XML, CSV or binary results.', name='Apache solr', url='hhttps://solr.apache.org/')},\n",
              " {'p': Node('tool', description='Lucene Core is a Java library providing powerful indexing and search features, as well as spellchecking, hit highlighting and advanced analysis/tokenization capabilities. The PyLucene sub project provides Python bindings for Lucene Core.', name='Apache lucene', url='https://lucene.apache.org/')},\n",
              " {'p': Node('tool', description='Visualize everything. Your stack for: monitoring, IoT visibility, LGTM (loki, graphana, tempo, mirmir), single pane of glass, K8s monitoring, observability or whatever you want to see.', name='graphana', url='https://grafana.com/')},\n",
              " {'p': Node('tool', description='pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.', name='pandas', url='https://pandas.pydata.org/')},\n",
              " {'p': Node('fileformat', description='', name='json')},\n",
              " {'p': Node('fileformat', description='The smallest, fastest columnar storage for Hadoop workloads (with ACID support).', name='Apache orc', url='https://orc.apache.org/')},\n",
              " {'p': Node('fileformat', description='', name='Apache arrow')},\n",
              " {'p': Node('fileformat', description='', name='Apache parquet')},\n",
              " {'p': Node('fileformat', description='Serialization of apache arrow tables. Can be instantiated by arrow tables without transformation.', name='Apache arrow files')},\n",
              " {'p': Node('fileformat', description='delta table is a secured parquet file format.', name='delta table', source='https://hevodata.com/learn/databricks-delta-tables/')},\n",
              " {'p': Node('fileformat', description='', name='avro')},\n",
              " {'p': Node('fileformat', description='', name='ORC')},\n",
              " {'p': Node('fileformat', description='Feather is a fast, lightweight, and easy-to-use binary file format for storing data frames (e.g. pandas) or Arrow tables.', name='feather', url='https://arrow.apache.org/docs/python/feather.html')},\n",
              " {'p': Node('deployment', name='managed service')},\n",
              " {'p': Node('deployment', name='embedded')},\n",
              " {'p': Node('deployment', name='local')},\n",
              " {'p': Node('deployment', name='kubernetes')},\n",
              " {'p': Node('deployment', name='self hosted server')},\n",
              " {'p': Node('deployment', name='desktop')},\n",
              " {'p': Node('filesystem', description='', name='hadoop FS')},\n",
              " {'p': Node('concept', name='graph database')},\n",
              " {'p': Node('concept', name='data science framework')},\n",
              " {'p': Node('concept', name='query language')},\n",
              " {'p': Node('concept', description='In Compositional engines such as Apache Storm, Samza, Apex the coding is at a lower level, as the user is explicitly defining the DAG, and could easily write a piece of inefficient code, but the code is at complete control of the developer.', name='compositional parallel processing engine', source='https://blog.scottlogic.com/2018/07/06/comparing-streaming-frameworks-pt1.html')},\n",
              " {'p': Node('concept', description='In Declarative engines such as Apache Spark and Flink the coding will look very functional, as is shown in the examples below. Plus the user may imply a DAG through their coding, which could be optimised by the engine.', name='declarative parallel processing engine', source='https://blog.scottlogic.com/2018/07/06/comparing-streaming-frameworks-pt1.html')},\n",
              " {'p': Node('concept', name='lakehouse')},\n",
              " {'p': Node('concept', name='data lake')},\n",
              " {'p': Node('concept', name='warehouse')},\n",
              " {'p': Node('concept', name='ETL')},\n",
              " {'p': Node('concept', description='', name='table format')},\n",
              " {'p': Node('concept', description='', name='map-reduce')},\n",
              " {'p': Node('concept', description='', name='parallel processing engine')},\n",
              " {'p': Node('concept', description='', name='column storage')},\n",
              " {'p': Node('concept', description='', name='OLTP')},\n",
              " {'p': Node('concept', description='', name='OLAP')},\n",
              " {'p': Node('concept', description='', name='schema evolution')},\n",
              " {'p': Node('concept', description='', name='time travel')},\n",
              " {'p': Node('concept', description='', name='row storage')},\n",
              " {'p': Node('concept', description='', name='database')},\n",
              " {'p': Node('concept', description='', name='metastore')},\n",
              " {'p': Node('concept', description='', name='git for data')},\n",
              " {'p': Node('concept', description='', name='Data mesh')},\n",
              " {'p': Node('concept', description='', name='memory layout')},\n",
              " {'p': Node('concept', description='', name='distributed SQL query engine')},\n",
              " {'p': Node('concept', description='It’s a server-side program which consumes data, builds it into a graph structure, and runs live computation on that graph to answer questions or compute results, and then stream them out.The main idea is two-sided: event-driven data <==> data-driven events', name='streaming graph interpreter', source='https://docs.quine.io/docs.html')},\n",
              " {'p': Node('concept', name='stream processing engine')},\n",
              " {'p': Node('concept', description='', name='batch processing')},\n",
              " {'p': Node('concept', description='', name='stream processing')},\n",
              " {'p': Node('concept', description='', name='search engine')},\n",
              " {'p': Node('concept', description='', name='visualization engine')},\n",
              " {'p': Node('concept', name='in-memory data structure')},\n",
              " {'p': Node('concept', description='', name='library')}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl \"https://raw.githubusercontent.com/jom-2021/test_cypher/main/DM%20Tools%20Database.CYPHER\" -o nj/import/Database.CYPHER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dN5bOmWIIHw",
        "outputId": "58e93214-4310-46fe-8f55-be7572b34469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 40771  100 40771    0     0   198k      0 --:--:-- --:--:-- --:--:--  198k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nj/bin/cypher-shell -f nj/import/Database.CYPHER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HplarpmxLlcD",
        "outputId": "619c0953-125a-4f61-8257-6ef84300943c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 rows\n",
            "ready to start consuming query after 264 ms, results consumed after another 0 ms\n",
            "Added 1 constraints\n",
            "0 rows\n",
            "ready to start consuming query after 87 ms, results consumed after another 0 ms\n",
            "Added 1 constraints\n",
            "0 rows\n",
            "ready to start consuming query after 72 ms, results consumed after another 0 ms\n",
            "Added 1 constraints\n",
            "0 rows\n",
            "ready to start consuming query after 44 ms, results consumed after another 0 ms\n",
            "0 rows\n",
            "ready to start consuming query after 270 ms, results consumed after another 0 ms\n",
            "Added 1 nodes, Set 2 properties, Added 2 labels\n",
            "0 rows\n",
            "ready to start consuming query after 165 ms, results consumed after another 0 ms\n",
            "Added 7 nodes, Set 17 properties, Added 14 labels\n",
            "0 rows\n",
            "ready to start consuming query after 232 ms, results consumed after another 0 ms\n",
            "Added 20 nodes, Set 41 properties, Added 20 labels\n",
            "0 rows\n",
            "ready to start consuming query after 284 ms, results consumed after another 0 ms\n",
            "Added 20 nodes, Set 52 properties, Added 20 labels\n",
            "0 rows\n",
            "ready to start consuming query after 187 ms, results consumed after another 0 ms\n",
            "Added 10 nodes, Set 30 properties, Added 10 labels\n",
            "0 rows\n",
            "ready to start consuming query after 122 ms, results consumed after another 0 ms\n",
            "Added 9 nodes, Set 30 properties, Added 18 labels\n",
            "0 rows\n",
            "ready to start consuming query after 81 ms, results consumed after another 0 ms\n",
            "Added 6 nodes, Set 12 properties, Added 12 labels\n",
            "0 rows\n",
            "ready to start consuming query after 57 ms, results consumed after another 0 ms\n",
            "Added 1 nodes, Set 3 properties, Added 2 labels\n",
            "0 rows\n",
            "ready to start consuming query after 149 ms, results consumed after another 0 ms\n",
            "Added 20 nodes, Set 35 properties, Added 20 labels\n",
            "0 rows\n",
            "ready to start consuming query after 118 ms, results consumed after another 0 ms\n",
            "Added 12 nodes, Set 23 properties, Added 12 labels\n",
            "0 rows\n",
            "ready to start consuming query after 418 ms, results consumed after another 0 ms\n",
            "Created 4 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 107 ms, results consumed after another 0 ms\n",
            "Created 1 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 130 ms, results consumed after another 0 ms\n",
            "Created 3 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 126 ms, results consumed after another 0 ms\n",
            "Created 4 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 225 ms, results consumed after another 0 ms\n",
            "Created 20 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 127 ms, results consumed after another 0 ms\n",
            "Created 5 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 105 ms, results consumed after another 0 ms\n",
            "Created 2 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 235 ms, results consumed after another 0 ms\n",
            "Created 20 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 104 ms, results consumed after another 0 ms\n",
            "Created 4 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 85 ms, results consumed after another 0 ms\n",
            "Created 1 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 91 ms, results consumed after another 0 ms\n",
            "Created 3 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 95 ms, results consumed after another 0 ms\n",
            "Created 1 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 189 ms, results consumed after another 0 ms\n",
            "Created 16 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 181 ms, results consumed after another 0 ms\n",
            "Created 4 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 95 ms, results consumed after another 0 ms\n",
            "Created 1 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 121 ms, results consumed after another 0 ms\n",
            "Created 4 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 222 ms, results consumed after another 0 ms\n",
            "Created 4 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 126 ms, results consumed after another 0 ms\n",
            "Created 1 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 73 ms, results consumed after another 0 ms\n",
            "Created 2 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 95 ms, results consumed after another 0 ms\n",
            "Created 2 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 150 ms, results consumed after another 0 ms\n",
            "Created 20 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 60 ms, results consumed after another 0 ms\n",
            "Created 20 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 116 ms, results consumed after another 0 ms\n",
            "Created 5 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 136 ms, results consumed after another 0 ms\n",
            "Created 12 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 177 ms, results consumed after another 0 ms\n",
            "Created 20 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 89 ms, results consumed after another 0 ms\n",
            "Created 2 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 80 ms, results consumed after another 0 ms\n",
            "Created 2 relationships\n",
            "0 rows\n",
            "ready to start consuming query after 106 ms, results consumed after another 0 ms\n",
            "Set 23 properties, Removed 23 labels\n",
            "0 rows\n",
            "ready to start consuming query after 27 ms, results consumed after another 0 ms\n",
            "Removed 1 constraints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cypher\n",
        "MATCH (p)-->() return count(p)\n",
        "//graph_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q39oSHLNjS8",
        "outputId": "5538d16d-8b2c-485a-e67a-3984db1f32b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'count(p)': 184}]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install py2neo "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VxW8qniNzKB",
        "outputId": "8cbdfec1-46c4-44db-bfaf-b6b232e228e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: py2neo in /usr/local/lib/python3.7/dist-packages (2021.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from py2neo) (2022.6.15)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from py2neo) (1.24.3)\n",
            "Requirement already satisfied: pansi>=2020.7.3 in /usr/local/lib/python3.7/dist-packages (from py2neo) (2020.7.3)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.7/dist-packages (from py2neo) (1.6)\n",
            "Requirement already satisfied: interchange~=2021.0.4 in /usr/local/lib/python3.7/dist-packages (from py2neo) (2021.0.4)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from py2neo) (2.6.1)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from py2neo) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from py2neo) (21.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from interchange~=2021.0.4->py2neo) (2022.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->py2neo) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from py2neo import Graph"
      ],
      "metadata": {
        "id": "xcuTGP5kO90Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = Graph(host=\"localhost\")"
      ],
      "metadata": {
        "id": "iX4H2ys-PAhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.run(\"MATCH (a)-[]->(b) RETURN a\").data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81EGrEI_PpJY",
        "outputId": "ffe36bae-5c16-4b7f-8f89-e4d8b321f043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'a': Node(name='cypher')},\n",
              " {'a': Node('is_retired', 'tool', description='', name='Apache apex')},\n",
              " {'a': Node('tool', description='', name='Aura DB')},\n",
              " {'a': Node('tool', description='', name='Aura DB')},\n",
              " {'a': Node('tool', description='', name='Aura DS')},\n",
              " {'a': Node('tool', description='', name='neo4j')},\n",
              " {'a': Node('tool', description='', name='neo4j')},\n",
              " {'a': Node('tool', description='', name='neo4j')},\n",
              " {'a': Node('tool', description='', name='neo4j')},\n",
              " {'a': Node('tool', description='', name='neo4j')},\n",
              " {'a': Node('tool', name='cypher')},\n",
              " {'a': Node('tool', name='Apache nessie')},\n",
              " {'a': Node('tool', name='Apache nessie')},\n",
              " {'a': Node('tool', name='Apache nessie')},\n",
              " {'a': Node('tool', name='Apache nessie')},\n",
              " {'a': Node('tool', description='Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.', name='Apache beam', url='https://beam.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.', name='Apache beam', url='https://beam.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.', name='Apache beam', url='https://beam.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.', name='Apache beam', url='https://beam.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.', name='Apache beam', url='https://beam.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.', name='Apache beam', url='https://beam.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.', name='Apache beam', url='https://beam.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.', name='Apache iceberg')},\n",
              " {'a': Node('tool', description='Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.', name='Apache iceberg')},\n",
              " {'a': Node('tool', description='Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.', name='Apache iceberg')},\n",
              " {'a': Node('tool', description='Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.', name='Apache iceberg')},\n",
              " {'a': Node('tool', description='Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.', name='Apache iceberg')},\n",
              " {'a': Node('tool', description='Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.', name='Apache iceberg')},\n",
              " {'a': Node('tool', description='Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table.', name='Apache iceberg')},\n",
              " {'a': Node('tool', description='Apache kafka is a middleware for the reliable transmission of events and for stream processing (kSQL). There exist a lot of connectors to connect with different data sources. Professional support (and more connectors and a managed service and a schema registry) is available from confluent.', name='Apache kafka')},\n",
              " {'a': Node('tool', description='Apache kafka is a middleware for the reliable transmission of events and for stream processing (kSQL). There exist a lot of connectors to connect with different data sources. Professional support (and more connectors and a managed service and a schema registry) is available from confluent.', name='Apache kafka')},\n",
              " {'a': Node('tool', description='Apache kafka is a middleware for the reliable transmission of events and for stream processing (kSQL). There exist a lot of connectors to connect with different data sources. Professional support (and more connectors and a managed service and a schema registry) is available from confluent.', name='Apache kafka')},\n",
              " {'a': Node('tool', description='Apache kafka is a middleware for the reliable transmission of events and for stream processing (kSQL). There exist a lot of connectors to connect with different data sources. Professional support (and more connectors and a managed service and a schema registry) is available from confluent.', name='Apache kafka')},\n",
              " {'a': Node('tool', description='Apache kafka is a middleware for the reliable transmission of events and for stream processing (kSQL). There exist a lot of connectors to connect with different data sources. Professional support (and more connectors and a managed service and a schema registry) is available from confluent.', name='Apache kafka')},\n",
              " {'a': Node('tool', description='Framework for the transmission of data on columnar format. It provides a protocol for the transmission without the need to convert it into rows (as it would be necessary when using odbc). It supports distributed processing.', name='Apache arrow flight')},\n",
              " {'a': Node('tool', description='Framework for the transmission of data on columnar format. It provides a protocol for the transmission without the need to convert it into rows (as it would be necessary when using odbc). It supports distributed processing.', name='Apache arrow flight')},\n",
              " {'a': Node('tool', description='Framework for the transmission of data on columnar format. It provides a protocol for the transmission without the need to convert it into rows (as it would be necessary when using odbc). It supports distributed processing.', name='Apache arrow flight')},\n",
              " {'a': Node('tool', description='Framework for the transmission of data on columnar format. It provides a protocol for the transmission without the need to convert it into rows (as it would be necessary when using odbc). It supports distributed processing.', name='Apache arrow flight')},\n",
              " {'a': Node('tool', description='Framework for the transmission of data on columnar format. It provides a protocol for the transmission without the need to convert it into rows (as it would be necessary when using odbc). It supports distributed processing.', name='Apache arrow flight')},\n",
              " {'a': Node('tool', description='Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.', name='Apache flink')},\n",
              " {'a': Node('tool', description='Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.', name='Apache flink')},\n",
              " {'a': Node('tool', description='Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.', name='Apache flink')},\n",
              " {'a': Node('tool', description='Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.', name='Apache flink')},\n",
              " {'a': Node('tool', description='Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.', name='Apache flink')},\n",
              " {'a': Node('tool', description='Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale.', name='Apache flink')},\n",
              " {'a': Node('tool', description='Apache Samza is a scalable data processing engine that allows you to process and analyze your data in real-time. Here is a summary of Samza’s features that simplify building your applications. Unified API: Use a simple API to describe your application-logic in a manner independent of your data-source. The same API can process both batch and streaming data.', name='Apache samza', url='https://samza.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Samza is a scalable data processing engine that allows you to process and analyze your data in real-time. Here is a summary of Samza’s features that simplify building your applications. Unified API: Use a simple API to describe your application-logic in a manner independent of your data-source. The same API can process both batch and streaming data.', name='Apache samza', url='https://samza.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Samza is a scalable data processing engine that allows you to process and analyze your data in real-time. Here is a summary of Samza’s features that simplify building your applications. Unified API: Use a simple API to describe your application-logic in a manner independent of your data-source. The same API can process both batch and streaming data.', name='Apache samza', url='https://samza.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Samza is a scalable data processing engine that allows you to process and analyze your data in real-time. Here is a summary of Samza’s features that simplify building your applications. Unified API: Use a simple API to describe your application-logic in a manner independent of your data-source. The same API can process both batch and streaming data.', name='Apache samza', url='https://samza.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Samza is a scalable data processing engine that allows you to process and analyze your data in real-time. Here is a summary of Samza’s features that simplify building your applications. Unified API: Use a simple API to describe your application-logic in a manner independent of your data-source. The same API can process both batch and streaming data.', name='Apache samza', url='https://samza.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Samza is a scalable data processing engine that allows you to process and analyze your data in real-time. Here is a summary of Samza’s features that simplify building your applications. Unified API: Use a simple API to describe your application-logic in a manner independent of your data-source. The same API can process both batch and streaming data.', name='Apache samza', url='https://samza.apache.org/')},\n",
              " {'a': Node('tool', description='Quine is built around a novel design choice: to represent both the data model and the computational model using a graph. In fact, the same graph is used for both models. Using the same model for both the data model and the computational model helps provide remarkable new capabilities from the Quine system.', name='Quine', url='https://docs.quine.io/core-concepts/core-concepts.html')},\n",
              " {'a': Node('tool', description='Quine is built around a novel design choice: to represent both the data model and the computational model using a graph. In fact, the same graph is used for both models. Using the same model for both the data model and the computational model helps provide remarkable new capabilities from the Quine system.', name='Quine', url='https://docs.quine.io/core-concepts/core-concepts.html')},\n",
              " {'a': Node('tool', description='Quine is built around a novel design choice: to represent both the data model and the computational model using a graph. In fact, the same graph is used for both models. Using the same model for both the data model and the computational model helps provide remarkable new capabilities from the Quine system.', name='Quine', url='https://docs.quine.io/core-concepts/core-concepts.html')},\n",
              " {'a': Node('tool', description='Snowflake is a fully managed SaaS (software as a service) that provides a single platform for data warehousing, data lakes, data engineering, data science, data application development, and secure sharing and consumption of real-time / shared data. Snowflake features out-of-the-box features like separation of storage and compute, on-the-fly scalable compute, data sharing, data cloning, and third-party tools support in order to handle the demanding needs of growing enterprises.', name='snowflake')},\n",
              " {'a': Node('tool', description='Snowflake is a fully managed SaaS (software as a service) that provides a single platform for data warehousing, data lakes, data engineering, data science, data application development, and secure sharing and consumption of real-time / shared data. Snowflake features out-of-the-box features like separation of storage and compute, on-the-fly scalable compute, data sharing, data cloning, and third-party tools support in order to handle the demanding needs of growing enterprises.', name='snowflake')},\n",
              " {'a': Node('tool', description='Snowflake is a fully managed SaaS (software as a service) that provides a single platform for data warehousing, data lakes, data engineering, data science, data application development, and secure sharing and consumption of real-time / shared data. Snowflake features out-of-the-box features like separation of storage and compute, on-the-fly scalable compute, data sharing, data cloning, and third-party tools support in order to handle the demanding needs of growing enterprises.', name='snowflake')},\n",
              " {'a': Node('tool', description='Snowflake is a fully managed SaaS (software as a service) that provides a single platform for data warehousing, data lakes, data engineering, data science, data application development, and secure sharing and consumption of real-time / shared data. Snowflake features out-of-the-box features like separation of storage and compute, on-the-fly scalable compute, data sharing, data cloning, and third-party tools support in order to handle the demanding needs of growing enterprises.', name='snowflake')},\n",
              " {'a': Node('tool', description='Snowflake is a fully managed SaaS (software as a service) that provides a single platform for data warehousing, data lakes, data engineering, data science, data application development, and secure sharing and consumption of real-time / shared data. Snowflake features out-of-the-box features like separation of storage and compute, on-the-fly scalable compute, data sharing, data cloning, and third-party tools support in order to handle the demanding needs of growing enterprises.', name='snowflake')},\n",
              " {'a': Node('tool', name='Elastic search')},\n",
              " {'a': Node('tool', name='Elastic search')},\n",
              " {'a': Node('tool', name='Elastic search')},\n",
              " {'a': Node('tool', name='Elastic search')},\n",
              " {'a': Node('tool', name='dremio')},\n",
              " {'a': Node('tool', name='dremio')},\n",
              " {'a': Node('tool', name='dremio')},\n",
              " {'a': Node('tool', name='dremio')},\n",
              " {'a': Node('tool', description='', name='jq-assistant')},\n",
              " {'a': Node('tool', description='', name='hadoop')},\n",
              " {'a': Node('tool', description='', name='hadoop')},\n",
              " {'a': Node('tool', description='', name='hadoop')},\n",
              " {'a': Node('tool', description='', name='hadoop')},\n",
              " {'a': Node('tool', description='Impala is a massively parallel processing engine that is an open source engine. It requires the database to be stored in clusters of computers that are running Apache Hadoop. It is a SQL engine, launched by Cloudera in 2012.', name='Apache impala', url='https://impala.apache.org/index.html')},\n",
              " {'a': Node('tool', description='Impala is a massively parallel processing engine that is an open source engine. It requires the database to be stored in clusters of computers that are running Apache Hadoop. It is a SQL engine, launched by Cloudera in 2012.', name='Apache impala', url='https://impala.apache.org/index.html')},\n",
              " {'a': Node('tool', description='Impala is a massively parallel processing engine that is an open source engine. It requires the database to be stored in clusters of computers that are running Apache Hadoop. It is a SQL engine, launched by Cloudera in 2012.', name='Apache impala', url='https://impala.apache.org/index.html')},\n",
              " {'a': Node('tool', description='Impala is a massively parallel processing engine that is an open source engine. It requires the database to be stored in clusters of computers that are running Apache Hadoop. It is a SQL engine, launched by Cloudera in 2012.', name='Apache impala', url='https://impala.apache.org/index.html')},\n",
              " {'a': Node('tool', description='Impala is a massively parallel processing engine that is an open source engine. It requires the database to be stored in clusters of computers that are running Apache Hadoop. It is a SQL engine, launched by Cloudera in 2012.', name='Apache impala', url='https://impala.apache.org/index.html')},\n",
              " {'a': Node('tool', description='Impala is a massively parallel processing engine that is an open source engine. It requires the database to be stored in clusters of computers that are running Apache Hadoop. It is a SQL engine, launched by Cloudera in 2012.', name='Apache impala', url='https://impala.apache.org/index.html')},\n",
              " {'a': Node('tool', description='Apache Hive is basically an open source data warehouse system which can handle, query, or analyze large datasets and process structured/non-structured data in Hadoop. Apache Hive is built on Hadoop big data platform.', name='hive')},\n",
              " {'a': Node('tool', description='Apache Hive is basically an open source data warehouse system which can handle, query, or analyze large datasets and process structured/non-structured data in Hadoop. Apache Hive is built on Hadoop big data platform.', name='hive')},\n",
              " {'a': Node('tool', description='Apache Hive is basically an open source data warehouse system which can handle, query, or analyze large datasets and process structured/non-structured data in Hadoop. Apache Hive is built on Hadoop big data platform.', name='hive')},\n",
              " {'a': Node('tool', description='Apache Hive is basically an open source data warehouse system which can handle, query, or analyze large datasets and process structured/non-structured data in Hadoop. Apache Hive is built on Hadoop big data platform.', name='hive')},\n",
              " {'a': Node('tool', description='Apache Hive is basically an open source data warehouse system which can handle, query, or analyze large datasets and process structured/non-structured data in Hadoop. Apache Hive is built on Hadoop big data platform.', name='hive')},\n",
              " {'a': Node('tool', description='Apache Hive is basically an open source data warehouse system which can handle, query, or analyze large datasets and process structured/non-structured data in Hadoop. Apache Hive is built on Hadoop big data platform.', name='hive')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.', name='Apache spark', url='https://spark.apache.org/docs/latest/index.html')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).', name='trino')},\n",
              " {'a': Node('tool', description='', name='prestoSQL')},\n",
              " {'a': Node('tool', description='Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.', do_not_use_for='analytics', name='Apache cassandra')},\n",
              " {'a': Node('tool', description='Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.', do_not_use_for='analytics', name='Apache cassandra')},\n",
              " {'a': Node('tool', description='Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.', do_not_use_for='analytics', name='Apache cassandra')},\n",
              " {'a': Node('tool', description='Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.', do_not_use_for='analytics', name='Apache cassandra')},\n",
              " {'a': Node('tool', description='Apache Pig[1] is a high-level platform for creating programs that run on Apache Hadoop. The language for this platform is called Pig Latin.[1] Pig can execute its Hadoop jobs in MapReduce, Apache Tez, or Apache Spark.[2] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for relational database management systems. Pig Latin can be extended using user-defined functions (UDFs) which the user can write in Java, Python, JavaScript, Ruby or Groovy[3] and then call directly from the language.', name='Apache pig', url='https://pig.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Pig[1] is a high-level platform for creating programs that run on Apache Hadoop. The language for this platform is called Pig Latin.[1] Pig can execute its Hadoop jobs in MapReduce, Apache Tez, or Apache Spark.[2] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for relational database management systems. Pig Latin can be extended using user-defined functions (UDFs) which the user can write in Java, Python, JavaScript, Ruby or Groovy[3] and then call directly from the language.', name='Apache pig', url='https://pig.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Pig[1] is a high-level platform for creating programs that run on Apache Hadoop. The language for this platform is called Pig Latin.[1] Pig can execute its Hadoop jobs in MapReduce, Apache Tez, or Apache Spark.[2] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for relational database management systems. Pig Latin can be extended using user-defined functions (UDFs) which the user can write in Java, Python, JavaScript, Ruby or Groovy[3] and then call directly from the language.', name='Apache pig', url='https://pig.apache.org/')},\n",
              " {'a': Node('tool', description='An intelligent metastore for Apache Iceberg that uniquely provides users a Git-like experience for data and automatically optimizes data to ensure high performance analytics.', name='dremio arctic')},\n",
              " {'a': Node('tool', description='An intelligent metastore for Apache Iceberg that uniquely provides users a Git-like experience for data and automatically optimizes data to ensure high performance analytics.', name='dremio arctic')},\n",
              " {'a': Node('tool', description='An intelligent metastore for Apache Iceberg that uniquely provides users a Git-like experience for data and automatically optimizes data to ensure high performance analytics.', name='dremio arctic')},\n",
              " {'a': Node('tool', description='An intelligent metastore for Apache Iceberg that uniquely provides users a Git-like experience for data and automatically optimizes data to ensure high performance analytics.', name='dremio arctic')},\n",
              " {'a': Node('tool', description='An intelligent metastore for Apache Iceberg that uniquely provides users a Git-like experience for data and automatically optimizes data to ensure high performance analytics.', name='dremio arctic')},\n",
              " {'a': Node('tool', description='A SQL engine for open platforms that provides data warehouse-level performance and capabilities on the data lake, and a self-service experience that makes data consumable and collaborative.', name='dremio sonar')},\n",
              " {'a': Node('tool', description='A SQL engine for open platforms that provides data warehouse-level performance and capabilities on the data lake, and a self-service experience that makes data consumable and collaborative.', name='dremio sonar')},\n",
              " {'a': Node('tool', description='', name='Power BI')},\n",
              " {'a': Node('tool', description='', name='Tableau')},\n",
              " {'a': Node('tool', description='', name='prestoDB')},\n",
              " {'a': Node('tool', description='Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto was designed and written from the ground up for interactive analytics and approaches the speed of commercial data warehouses while scaling to the size of organizations like Facebook.', name='presto', url='https://prestodb.io/')},\n",
              " {'a': Node('tool', description='Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto was designed and written from the ground up for interactive analytics and approaches the speed of commercial data warehouses while scaling to the size of organizations like Facebook.', name='presto', url='https://prestodb.io/')},\n",
              " {'a': Node('tool', description='Apache Hudi (pronounced “hoodie”) is the next generation streaming data lake platform. Apache Hudi brings core warehouse and database functionality directly to a data lake. Hudi provides tables, transactions, efficient upserts/deletes, advanced indexes, streaming ingestion services, data clustering/compaction optimizations, and concurrency all while keeping your data in open source file formats.', name='Apache hudi', url='https://hudi.apache.org/docs/overview')},\n",
              " {'a': Node('tool', description='Apache Hudi (pronounced “hoodie”) is the next generation streaming data lake platform. Apache Hudi brings core warehouse and database functionality directly to a data lake. Hudi provides tables, transactions, efficient upserts/deletes, advanced indexes, streaming ingestion services, data clustering/compaction optimizations, and concurrency all while keeping your data in open source file formats.', name='Apache hudi', url='https://hudi.apache.org/docs/overview')},\n",
              " {'a': Node('tool', description='Apache Hudi (pronounced “hoodie”) is the next generation streaming data lake platform. Apache Hudi brings core warehouse and database functionality directly to a data lake. Hudi provides tables, transactions, efficient upserts/deletes, advanced indexes, streaming ingestion services, data clustering/compaction optimizations, and concurrency all while keeping your data in open source file formats.', name='Apache hudi', url='https://hudi.apache.org/docs/overview')},\n",
              " {'a': Node('tool', description='Apache Hudi (pronounced “hoodie”) is the next generation streaming data lake platform. Apache Hudi brings core warehouse and database functionality directly to a data lake. Hudi provides tables, transactions, efficient upserts/deletes, advanced indexes, streaming ingestion services, data clustering/compaction optimizations, and concurrency all while keeping your data in open source file formats.', name='Apache hudi', url='https://hudi.apache.org/docs/overview')},\n",
              " {'a': Node('tool', description='Apache Hudi (pronounced “hoodie”) is the next generation streaming data lake platform. Apache Hudi brings core warehouse and database functionality directly to a data lake. Hudi provides tables, transactions, efficient upserts/deletes, advanced indexes, streaming ingestion services, data clustering/compaction optimizations, and concurrency all while keeping your data in open source file formats.', name='Apache hudi', url='https://hudi.apache.org/docs/overview')},\n",
              " {'a': Node('tool', description='Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.', name='delta lake', url='https://delta.io')},\n",
              " {'a': Node('tool', description='Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.', name='delta lake', url='https://delta.io')},\n",
              " {'a': Node('tool', description='Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.', name='delta lake', url='https://delta.io')},\n",
              " {'a': Node('tool', description='Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.', name='delta lake', url='https://delta.io')},\n",
              " {'a': Node('tool', description='Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.', name='delta lake', url='https://delta.io')},\n",
              " {'a': Node('tool', description='Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.', name='delta lake', url='https://delta.io')},\n",
              " {'a': Node('tool', description='Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.', name='delta lake', url='https://delta.io')},\n",
              " {'a': Node('tool', description='Apache Storm is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Apache Storm is simple, can be used with any programming language, and is a lot of fun to use! Apache Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Apache Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate. Apache Storm integrates with the queueing and database technologies you already use. An Apache Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed.', name='Apache storm', url='https://storm.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Storm is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Apache Storm is simple, can be used with any programming language, and is a lot of fun to use! Apache Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Apache Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate. Apache Storm integrates with the queueing and database technologies you already use. An Apache Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed.', name='Apache storm', url='https://storm.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Storm is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Apache Storm is simple, can be used with any programming language, and is a lot of fun to use! Apache Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Apache Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate. Apache Storm integrates with the queueing and database technologies you already use. An Apache Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed.', name='Apache storm', url='https://storm.apache.org/')},\n",
              " {'a': Node('tool', description='Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.', name='Apache spark Structured Streaming', url='https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html')},\n",
              " {'a': Node('tool', description='Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.', name='Apache spark Structured Streaming', url='https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html')},\n",
              " {'a': Node('tool', description='Drill is a distributed query engine that doesnt require schemas. The data structure is infered and queries can span multiple sources. Sources are processed in-situ. The service can run on a single laptop or can scale via hadoop. Yarn is leveraged for the ressource management. It provides a standard SQL query language and supports common BI tools like tableau or PowerBI.', name='Apache drill', url='https://drill.apache.org/')},\n",
              " {'a': Node('tool', description='Drill is a distributed query engine that doesnt require schemas. The data structure is infered and queries can span multiple sources. Sources are processed in-situ. The service can run on a single laptop or can scale via hadoop. Yarn is leveraged for the ressource management. It provides a standard SQL query language and supports common BI tools like tableau or PowerBI.', name='Apache drill', url='https://drill.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'a': Node('tool', description='Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.', name='Apache druid', url='https://druid.apache.org/')},\n",
              " {'a': Node('tool', description='Teradata Vantage is a platform for data analytics. It is based on a RDBMS. It allows installation in multiple public and private clouds.', name='teradata Vantage', url='https://www.teradata.de/Vantage')},\n",
              " {'a': Node('tool', description='Teradata Vantage is a platform for data analytics. It is based on a RDBMS. It allows installation in multiple public and private clouds.', name='teradata Vantage', url='https://www.teradata.de/Vantage')},\n",
              " {'a': Node('tool', description='Teradata Vantage is a platform for data analytics. It is based on a RDBMS. It allows installation in multiple public and private clouds.', name='teradata Vantage', url='https://www.teradata.de/Vantage')},\n",
              " {'a': Node('tool', description='In a class by itself, only Apache HAWQ combines exceptional MPP-based analytics performance, robust ANSI SQL compliance, Hadoop ecosystem integration and manageability, and flexible data-store format support. All natively in Apache Hadoop. No connectors required.', name='Apache hawq', url='https://hawq.apache.org/')},\n",
              " {'a': Node('tool', description='In a class by itself, only Apache HAWQ combines exceptional MPP-based analytics performance, robust ANSI SQL compliance, Hadoop ecosystem integration and manageability, and flexible data-store format support. All natively in Apache Hadoop. No connectors required.', name='Apache hawq', url='https://hawq.apache.org/')},\n",
              " {'a': Node('tool', description='In a class by itself, only Apache HAWQ combines exceptional MPP-based analytics performance, robust ANSI SQL compliance, Hadoop ecosystem integration and manageability, and flexible data-store format support. All natively in Apache Hadoop. No connectors required.', name='Apache hawq', url='https://hawq.apache.org/')},\n",
              " {'a': Node('tool', description='In a class by itself, only Apache HAWQ combines exceptional MPP-based analytics performance, robust ANSI SQL compliance, Hadoop ecosystem integration and manageability, and flexible data-store format support. All natively in Apache Hadoop. No connectors required.', name='Apache hawq', url='https://hawq.apache.org/')},\n",
              " {'a': Node('tool', description=\"Apache HBase™ is the Hadoop database, a distributed, scalable, big data store. Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.\", name='Apache hbase', url='https://hbase.apache.org/')},\n",
              " {'a': Node('tool', description=\"Apache HBase™ is the Hadoop database, a distributed, scalable, big data store. Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.\", name='Apache hbase', url='https://hbase.apache.org/')},\n",
              " {'a': Node('tool', description=\"Apache HBase™ is the Hadoop database, a distributed, scalable, big data store. Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.\", name='Apache hbase', url='https://hbase.apache.org/')},\n",
              " {'a': Node('tool', description=\"Apache HBase™ is the Hadoop database, a distributed, scalable, big data store. Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.\", name='Apache hbase', url='https://hbase.apache.org/')},\n",
              " {'a': Node('tool', description=\"Apache HBase™ is the Hadoop database, a distributed, scalable, big data store. Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.\", name='Apache hbase', url='https://hbase.apache.org/')},\n",
              " {'a': Node('tool', description=\"Apache HBase™ is the Hadoop database, a distributed, scalable, big data store. Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.\", name='Apache hbase', url='https://hbase.apache.org/')},\n",
              " {'a': Node('tool', description='Apache nemo is a framework to optimise the scheduling and communication of distributed data processing. It places itself between Applications (e.g. on Apache beam) and the distributed parallel processing engines (such like Apache spark). https://www.usenix.org/system/files/atc19-yang-youngseok.pdf and https://www.usenix.org/conference/atc19/presentation/yang-youngseok', name='Apache nemo', url='https://nemo.apache.org/docs/home/')},\n",
              " {'a': Node('tool', description='Airflow is a platform created by the community to programmatically author, schedule and monitor workflows. Apache Airflow is an open-source WMS designed for authoring, scheduling, and monitoring workflows as DAGs (directed acyclic graphs). Workflows are written in Python, which allows flexible interaction with third-party APIs, databases, and data systems. Data pipelines in Airflow are built by defining a set of tasks to extract, transform, load, analyze or store the data. Airflow is a workflow scheduler designed to help with scheduling complex workflows and provide them an easy way to maintain them. It is a great product for data engineering if you use it with the purpose it was designed for – to orchestrate work executed on external systems such as Spark, Hadoop, Druid, cloud services, etc. For example, if your task is to load data in PostgreSQL, make some aggregations using Spark, and store the data on your Hadoop cluster (like in Figure 2.), then Airflow is the best choice since it can tie many external systems together. Airflow was not designed to execute any workflows directly inside but to schedule them and keep the execution within external systems.', name='Apache airflow', url='https://airflow.apache.org/')},\n",
              " {'a': Node('tool', description='Airflow is a platform created by the community to programmatically author, schedule and monitor workflows. Apache Airflow is an open-source WMS designed for authoring, scheduling, and monitoring workflows as DAGs (directed acyclic graphs). Workflows are written in Python, which allows flexible interaction with third-party APIs, databases, and data systems. Data pipelines in Airflow are built by defining a set of tasks to extract, transform, load, analyze or store the data. Airflow is a workflow scheduler designed to help with scheduling complex workflows and provide them an easy way to maintain them. It is a great product for data engineering if you use it with the purpose it was designed for – to orchestrate work executed on external systems such as Spark, Hadoop, Druid, cloud services, etc. For example, if your task is to load data in PostgreSQL, make some aggregations using Spark, and store the data on your Hadoop cluster (like in Figure 2.), then Airflow is the best choice since it can tie many external systems together. Airflow was not designed to execute any workflows directly inside but to schedule them and keep the execution within external systems.', name='Apache airflow', url='https://airflow.apache.org/')},\n",
              " {'a': Node('tool', description='Solr is a standalone enterprise search server with a REST-like API. You put documents in it (called \"indexing\") via JSON, XML, CSV or binary over HTTP. You query it via HTTP GET and receive JSON, XML, CSV or binary results.', name='Apache solr', url='hhttps://solr.apache.org/')},\n",
              " {'a': Node('tool', description='Solr is a standalone enterprise search server with a REST-like API. You put documents in it (called \"indexing\") via JSON, XML, CSV or binary over HTTP. You query it via HTTP GET and receive JSON, XML, CSV or binary results.', name='Apache solr', url='hhttps://solr.apache.org/')},\n",
              " {'a': Node('tool', description='Solr is a standalone enterprise search server with a REST-like API. You put documents in it (called \"indexing\") via JSON, XML, CSV or binary over HTTP. You query it via HTTP GET and receive JSON, XML, CSV or binary results.', name='Apache solr', url='hhttps://solr.apache.org/')},\n",
              " {'a': Node('tool', description='Visualize everything. Your stack for: monitoring, IoT visibility, LGTM (loki, graphana, tempo, mirmir), single pane of glass, K8s monitoring, observability or whatever you want to see.', name='graphana', url='https://grafana.com/')},\n",
              " {'a': Node('tool', description='pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.', name='pandas', url='https://pandas.pydata.org/')},\n",
              " {'a': Node('tool', description='pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.', name='pandas', url='https://pandas.pydata.org/')},\n",
              " {'a': Node('fileformat', description='The smallest, fastest columnar storage for Hadoop workloads (with ACID support).', name='Apache orc', url='https://orc.apache.org/')},\n",
              " {'a': Node('fileformat', description='', name='Apache arrow')},\n",
              " {'a': Node('fileformat', description='', name='Apache arrow')},\n",
              " {'a': Node('fileformat', description='', name='Apache arrow')},\n",
              " {'a': Node('fileformat', description='', name='Apache arrow')},\n",
              " {'a': Node('fileformat', description='', name='Apache parquet')},\n",
              " {'a': Node('fileformat', description='', name='Apache parquet')},\n",
              " {'a': Node('fileformat', description='', name='Apache parquet')},\n",
              " {'a': Node('fileformat', description='Serialization of apache arrow tables. Can be instantiated by arrow tables without transformation.', name='Apache arrow files')},\n",
              " {'a': Node('concept', description='In Compositional engines such as Apache Storm, Samza, Apex the coding is at a lower level, as the user is explicitly defining the DAG, and could easily write a piece of inefficient code, but the code is at complete control of the developer.', name='compositional parallel processing engine', source='https://blog.scottlogic.com/2018/07/06/comparing-streaming-frameworks-pt1.html')},\n",
              " {'a': Node('concept', description='In Declarative engines such as Apache Spark and Flink the coding will look very functional, as is shown in the examples below. Plus the user may imply a DAG through their coding, which could be optimised by the engine.', name='declarative parallel processing engine', source='https://blog.scottlogic.com/2018/07/06/comparing-streaming-frameworks-pt1.html')}]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}