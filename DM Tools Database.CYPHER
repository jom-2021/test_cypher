:begin
CREATE CONSTRAINT ON (node:tool) ASSERT (node.name) IS UNIQUE;
CREATE CONSTRAINT ON (node:concept) ASSERT (node.name) IS UNIQUE;
CREATE CONSTRAINT ON (node:`UNIQUE IMPORT LABEL`) ASSERT (node.`UNIQUE IMPORT ID`) IS UNIQUE;
:commit
CALL db.awaitIndexes(300);
:begin
UNWIND [{name:"Apache apex", properties:{description:""}}] AS row
CREATE (n:tool{name: row.name}) SET n += row.properties SET n:is_retired;
UNWIND [{_id:19, properties:{name:"databricks"}}, {_id:37, properties:{name:"facebook", description:""}}, {_id:38, properties:{name:"apache", description:""}}, {_id:39, properties:{name:"cloudera", description:""}}, {_id:84, properties:{name:"linkedIn"}}, {_id:96, properties:{name:"AirBnB"}}, {_id:97, properties:{name:"Lucene"}}] AS row
CREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:origin;
UNWIND [{name:"Aura DB", properties:{description:""}}, {name:"Aura DS", properties:{description:""}}, {name:"neo4j", properties:{description:""}}, {name:"cypher", properties:{}}, {name:"Apache nessie", properties:{}}, {name:"Apache beam", properties:{description:"Apache Beam is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open source Beam SDKs, you build a program that defines the pipeline. The pipeline is then executed by one of Beam’s supported distributed processing back-ends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.", url:"https://beam.apache.org/"}}, {name:"Apache iceberg", properties:{description:"Apache Iceberg is an open table format for huge analytic datasets. Iceberg adds tables to compute engines including Spark, Trino, PrestoDB, Flink, Hive and Impala using a high-performance table format that works just like a SQL table."}}, {name:"Apache kafka", properties:{description:"Apache kafka is a middleware for the reliable transmission of events and for stream processing (kSQL). There exist a lot of connectors to connect with different data sources. Professional support (and more connectors and a managed service and a schema registry) is available from confluent."}}, {name:"Apache arrow flight", properties:{description:"Framework for the transmission of data on columnar format. It provides a protocol for the transmission without the need to convert it into rows (as it would be necessary when using odbc). It supports distributed processing."}}, {name:"Apache flink", properties:{description:"Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. Flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale."}}, {name:"Apache samza", properties:{description:"Apache Samza is a scalable data processing engine that allows you to process and analyze your data in real-time. Here is a summary of Samza’s features that simplify building your applications. Unified API: Use a simple API to describe your application-logic in a manner independent of your data-source. The same API can process both batch and streaming data.", url:"https://samza.apache.org/"}}, {name:"Quine", properties:{description:"Quine is built around a novel design choice: to represent both the data model and the computational model using a graph. In fact, the same graph is used for both models. Using the same model for both the data model and the computational model helps provide remarkable new capabilities from the Quine system.", url:"https://docs.quine.io/core-concepts/core-concepts.html"}}, {name:"snowflake", properties:{description:"Snowflake is a fully managed SaaS (software as a service) that provides a single platform for data warehousing, data lakes, data engineering, data science, data application development, and secure sharing and consumption of real-time / shared data. Snowflake features out-of-the-box features like separation of storage and compute, on-the-fly scalable compute, data sharing, data cloning, and third-party tools support in order to handle the demanding needs of growing enterprises."}}, {name:"Elastic search", properties:{}}, {name:"dremio", properties:{}}, {name:"yarn", properties:{description:"Yarn is a package manager that doubles down as project manager. Whether you work on one-shot projects or large monorepos, as a hobbyist or an enterprise user, we have got you covered.", url:"https://yarnpkg.com/"}}, {name:"jq-assistant", properties:{description:""}}, {name:"hadoop", properties:{description:""}}, {name:"Apache impala", properties:{description:"Impala is a massively parallel processing engine that is an open source engine. It requires the database to be stored in clusters of computers that are running Apache Hadoop. It is a SQL engine, launched by Cloudera in 2012.", url:"https://impala.apache.org/index.html"}}, {name:"hive", properties:{description:"Apache Hive is basically an open source data warehouse system which can handle, query, or analyze large datasets and process structured/non-structured data in Hadoop. Apache Hive is built on Hadoop big data platform."}}] AS row
CREATE (n:tool{name: row.name}) SET n += row.properties;
UNWIND [{name:"Apache spark", properties:{description:"It is a general-purpose data processing engine. There are lots of additional libraries on the top of core spark data processing like graph computation, machine learning and stream processing. These libraries can be used together in an application. Spark supports the following languages like Spark, Java and R application development.", url:"https://spark.apache.org/docs/latest/index.html"}}, {name:"trino", properties:{description:"Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP)."}}, {name:"prestoSQL", properties:{description:""}}, {name:"Apache cassandra", properties:{do_not_use_for:"analytics", description:"Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data."}}, {name:"Apache pig", properties:{description:"Apache Pig[1] is a high-level platform for creating programs that run on Apache Hadoop. The language for this platform is called Pig Latin.[1] Pig can execute its Hadoop jobs in MapReduce, Apache Tez, or Apache Spark.[2] Pig Latin abstracts the programming from the Java MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of SQL for relational database management systems. Pig Latin can be extended using user-defined functions (UDFs) which the user can write in Java, Python, JavaScript, Ruby or Groovy[3] and then call directly from the language.", url:"https://pig.apache.org/"}}, {name:"dremio arctic", properties:{description:"An intelligent metastore for Apache Iceberg that uniquely provides users a Git-like experience for data and automatically optimizes data to ensure high performance analytics."}}, {name:"dremio sonar", properties:{description:"A SQL engine for open platforms that provides data warehouse-level performance and capabilities on the data lake, and a self-service experience that makes data consumable and collaborative."}}, {name:"Power BI", properties:{description:""}}, {name:"Tableau", properties:{description:""}}, {name:"Apache arrow flight sql", properties:{description:"Developer-friendly interface for connecting to data systems with Arrow Flight endpoints (like odbc/jdbc)."}}, {name:"prestoDB", properties:{description:""}}, {name:"presto", properties:{description:"Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto was designed and written from the ground up for interactive analytics and approaches the speed of commercial data warehouses while scaling to the size of organizations like Facebook.", url:"https://prestodb.io/"}}, {name:"Apache hudi", properties:{description:"Apache Hudi (pronounced “hoodie”) is the next generation streaming data lake platform. Apache Hudi brings core warehouse and database functionality directly to a data lake. Hudi provides tables, transactions, efficient upserts/deletes, advanced indexes, streaming ingestion services, data clustering/compaction optimizations, and concurrency all while keeping your data in open source file formats.", url:"https://hudi.apache.org/docs/overview"}}, {name:"delta lake", properties:{description:"Delta Lake is an open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.", url:"https://delta.io"}}, {name:"Apache storm", properties:{description:"Apache Storm is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing. Apache Storm is simple, can be used with any programming language, and is a lot of fun to use! Apache Storm has many use cases: realtime analytics, online machine learning, continuous computation, distributed RPC, ETL, and more. Apache Storm is fast: a benchmark clocked it at over a million tuples processed per second per node. It is scalable, fault-tolerant, guarantees your data will be processed, and is easy to set up and operate. Apache Storm integrates with the queueing and database technologies you already use. An Apache Storm topology consumes streams of data and processes those streams in arbitrarily complex ways, repartitioning the streams between each stage of the computation however needed.", url:"https://storm.apache.org/"}}, {name:"Apache spark SQL", properties:{description:"Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, Spark SQL uses this extra information to perform extra optimizations. There are several ways to interact with Spark SQL including SQL and the Dataset API. When computing a result, the same execution engine is used, independent of which API/language you are using to express the computation. This unification means that developers can easily switch back and forth between different APIs based on which provides the most natural way to express a given transformation.", url:"https://spark.apache.org/docs/latest/sql-programming-guide.html"}}, {name:"Apache spark Structured Streaming", properties:{description:"Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.", url:"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"}}, {name:"Apache spark GraphX", properties:{description:"GraphX is a new component in Spark for graphs and graph-parallel computation. At a high level, GraphX extends the Spark RDD by introducing a new Graph abstraction: a directed multigraph with properties attached to each vertex and edge. To support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and aggregateMessages) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.", url:"https://spark.apache.org/docs/latest/graphx-programming-guide.html#overview"}}, {name:"Apache drill", properties:{description:"Drill is a distributed query engine that doesnt require schemas. The data structure is infered and queries can span multiple sources. Sources are processed in-situ. The service can run on a single laptop or can scale via hadoop. Yarn is leveraged for the ressource management. It provides a standard SQL query language and supports common BI tools like tableau or PowerBI.", url:"https://drill.apache.org/"}}, {name:"Apache druid", properties:{description:"Apache Druid is an open source distributed data store. Druid’s core design combines ideas from data warehouses, timeseries databases, and search systems to create a high performance real-time analytics database for a broad range of use cases. Apache Druid is a database that is most often used for powering use cases where real-time ingest, fast query performance, and high uptime are important. As such, Druid is commonly used for powering GUIs of analytical applications, or as a backend for highly-concurrent APIs that need fast aggregations. Druid works best with event-oriented data.", url:"https://druid.apache.org/"}}] AS row
CREATE (n:tool{name: row.name}) SET n += row.properties;
UNWIND [{name:"teradata Vantage", properties:{description:"Teradata Vantage is a platform for data analytics. It is based on a RDBMS. It allows installation in multiple public and private clouds.", url:"https://www.teradata.de/Vantage"}}, {name:"Apache hawq", properties:{description:"In a class by itself, only Apache HAWQ combines exceptional MPP-based analytics performance, robust ANSI SQL compliance, Hadoop ecosystem integration and manageability, and flexible data-store format support. All natively in Apache Hadoop. No connectors required.", url:"https://hawq.apache.org/"}}, {name:"Apache hbase", properties:{description:"Apache HBase™ is the Hadoop database, a distributed, scalable, big data store. Use Apache HBase™ when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google's Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.", url:"https://hbase.apache.org/"}}, {name:"Apache nemo", properties:{description:"Apache nemo is a framework to optimise the scheduling and communication of distributed data processing. It places itself between Applications (e.g. on Apache beam) and the distributed parallel processing engines (such like Apache spark). https://www.usenix.org/system/files/atc19-yang-youngseok.pdf and https://www.usenix.org/conference/atc19/presentation/yang-youngseok", url:"https://nemo.apache.org/docs/home/"}}, {name:"Apache airflow", properties:{description:"Airflow is a platform created by the community to programmatically author, schedule and monitor workflows. Apache Airflow is an open-source WMS designed for authoring, scheduling, and monitoring workflows as DAGs (directed acyclic graphs). Workflows are written in Python, which allows flexible interaction with third-party APIs, databases, and data systems. Data pipelines in Airflow are built by defining a set of tasks to extract, transform, load, analyze or store the data. Airflow is a workflow scheduler designed to help with scheduling complex workflows and provide them an easy way to maintain them. It is a great product for data engineering if you use it with the purpose it was designed for – to orchestrate work executed on external systems such as Spark, Hadoop, Druid, cloud services, etc. For example, if your task is to load data in PostgreSQL, make some aggregations using Spark, and store the data on your Hadoop cluster (like in Figure 2.), then Airflow is the best choice since it can tie many external systems together. Airflow was not designed to execute any workflows directly inside but to schedule them and keep the execution within external systems.", url:"https://airflow.apache.org/"}}, {name:"hasura", properties:{description:"Instant GraphQL on all your data. Run Hasura, locally or in the cloud, and connect it to your new or existing databases to instantly get a production grade GraphQL API.", url:"https://hasura.io/"}}, {name:"Apache solr", properties:{description:"Solr is a standalone enterprise search server with a REST-like API. You put documents in it (called \"indexing\") via JSON, XML, CSV or binary over HTTP. You query it via HTTP GET and receive JSON, XML, CSV or binary results.", url:"hhttps://solr.apache.org/"}}, {name:"Apache lucene", properties:{description:"Lucene Core is a Java library providing powerful indexing and search features, as well as spellchecking, hit highlighting and advanced analysis/tokenization capabilities. The PyLucene sub project provides Python bindings for Lucene Core.", url:"https://lucene.apache.org/"}}, {name:"graphana", properties:{description:"Visualize everything. Your stack for: monitoring, IoT visibility, LGTM (loki, graphana, tempo, mirmir), single pane of glass, K8s monitoring, observability or whatever you want to see.", url:"https://grafana.com/"}}, {name:"pandas", properties:{description:"pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.", url:"https://pandas.pydata.org/"}}] AS row
CREATE (n:tool{name: row.name}) SET n += row.properties;
UNWIND [{_id:12, properties:{name:"json", description:""}}, {_id:21, properties:{name:"Apache orc", description:"The smallest, fastest columnar storage for Hadoop workloads (with ACID support).", url:"https://orc.apache.org/"}}, {_id:64, properties:{name:"Apache arrow", description:""}}, {_id:65, properties:{name:"Apache parquet", description:""}}, {_id:67, properties:{name:"Apache arrow files", description:"Serialization of apache arrow tables. Can be instantiated by arrow tables without transformation."}}, {_id:68, properties:{name:"delta table", description:"delta table is a secured parquet file format.", source:"https://hevodata.com/learn/databricks-delta-tables/"}}, {_id:75, properties:{name:"avro", description:""}}, {_id:76, properties:{name:"ORC", description:""}}, {_id:95, properties:{name:"feather", description:"Feather is a fast, lightweight, and easy-to-use binary file format for storing data frames (e.g. pandas) or Arrow tables.", url:"https://arrow.apache.org/docs/python/feather.html"}}] AS row
CREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:fileformat;
UNWIND [{_id:22, properties:{name:"managed service"}}, {_id:23, properties:{name:"embedded"}}, {_id:24, properties:{name:"local"}}, {_id:25, properties:{name:"kubernetes"}}, {_id:26, properties:{name:"self hosted server"}}, {_id:27, properties:{name:"desktop"}}] AS row
CREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:deployment;
UNWIND [{_id:47, properties:{name:"hadoop FS", description:""}}] AS row
CREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:filesystem;
UNWIND [{name:"graph database", properties:{}}, {name:"data science framework", properties:{}}, {name:"query language", properties:{}}, {name:"compositional parallel processing engine", properties:{description:"In Compositional engines such as Apache Storm, Samza, Apex the coding is at a lower level, as the user is explicitly defining the DAG, and could easily write a piece of inefficient code, but the code is at complete control of the developer.", source:"https://blog.scottlogic.com/2018/07/06/comparing-streaming-frameworks-pt1.html"}}, {name:"declarative parallel processing engine", properties:{description:"In Declarative engines such as Apache Spark and Flink the coding will look very functional, as is shown in the examples below. Plus the user may imply a DAG through their coding, which could be optimised by the engine.", source:"https://blog.scottlogic.com/2018/07/06/comparing-streaming-frameworks-pt1.html"}}, {name:"lakehouse", properties:{}}, {name:"data lake", properties:{}}, {name:"warehouse", properties:{}}, {name:"ETL", properties:{}}, {name:"table format", properties:{description:""}}, {name:"map-reduce", properties:{description:""}}, {name:"parallel processing engine", properties:{description:""}}, {name:"column storage", properties:{description:""}}, {name:"OLTP", properties:{description:""}}, {name:"OLAP", properties:{description:""}}, {name:"schema evolution", properties:{description:""}}, {name:"time travel", properties:{description:""}}, {name:"row storage", properties:{description:""}}, {name:"database", properties:{description:""}}, {name:"metastore", properties:{description:""}}] AS row
CREATE (n:concept{name: row.name}) SET n += row.properties;
UNWIND [{name:"git for data", properties:{description:""}}, {name:"Data mesh", properties:{description:""}}, {name:"memory layout", properties:{description:""}}, {name:"distributed SQL query engine", properties:{description:""}}, {name:"streaming graph interpreter", properties:{description:"It’s a server-side program which consumes data, builds it into a graph structure, and runs live computation on that graph to answer questions or compute results, and then stream them out.The main idea is two-sided: event-driven data <==> data-driven events", source:"https://docs.quine.io/docs.html"}}, {name:"stream processing engine", properties:{}}, {name:"batch processing", properties:{description:""}}, {name:"stream processing", properties:{description:""}}, {name:"search engine", properties:{description:""}}, {name:"visualization engine", properties:{description:""}}, {name:"in-memory data structure", properties:{}}, {name:"library", properties:{description:""}}] AS row
CREATE (n:concept{name: row.name}) SET n += row.properties;
:commit
:begin
UNWIND [{start: {name:"neo4j"}, end: {_id:22}, properties:{}}, {start: {name:"neo4j"}, end: {_id:23}, properties:{}}, {start: {name:"neo4j"}, end: {_id:27}, properties:{}}, {start: {name:"snowflake"}, end: {_id:22}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:DEPLOYMENT_OPTION]->(end) SET r += row.properties;
UNWIND [{start: {_id:21}, end: {_id:38}, properties:{}}] AS row
MATCH (start:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.start._id})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:FROM]->(end) SET r += row.properties;
UNWIND [{start: {name:"snowflake"}, end: {_id:12}, properties:{}}, {start: {name:"snowflake"}, end: {_id:75}, properties:{}}, {start: {name:"snowflake"}, end: {_id:65}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:SUPPORT]->(end) SET r += row.properties;
UNWIND [{start: {name:"neo4j"}, end: {name:"Aura DB"}, properties:{}}, {start: {name:"neo4j"}, end: {name:"Aura DS"}, properties:{}}, {start: {name:"dremio"}, end: {name:"dremio sonar"}, properties:{}}, {start: {name:"dremio"}, end: {name:"dremio arctic"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:tool{name: row.end.name})
CREATE (start)-[r:CONSISTS_OF]->(end) SET r += row.properties;
UNWIND [{start: {name:"hadoop"}, end: {name:"map-reduce"}, properties:{}}, {start: {name:"trino"}, end: {name:"OLTP"}, properties:{}}, {start: {name:"trino"}, end: {name:"OLAP"}, properties:{}}, {start: {name:"trino"}, end: {name:"map-reduce"}, properties:{}}, {start: {name:"Apache iceberg"}, end: {name:"schema evolution"}, properties:{}}, {start: {name:"Apache iceberg"}, end: {name:"time travel"}, properties:{}}, {start: {name:"Apache hudi"}, end: {name:"time travel"}, properties:{}}, {start: {name:"Apache hudi"}, end: {name:"schema evolution"}, properties:{}}, {start: {name:"delta lake"}, end: {name:"schema evolution"}, properties:{}}, {start: {name:"delta lake"}, end: {name:"time travel"}, properties:{}}, {start: {name:"Apache pig"}, end: {name:"map-reduce"}, properties:{}}, {start: {name:"Apache kafka"}, end: {name:"schema evolution"}, properties:{}}, {start: {name:"Apache druid"}, end: {name:"schema evolution"}, properties:{}}, {start: {name:"Apache druid"}, end: {name:"time travel"}, properties:{}}, {start: {name:"Apache druid"}, end: {name:"batch processing"}, properties:{}}, {start: {name:"Apache druid"}, end: {name:"stream processing"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"stream processing"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"batch processing"}, properties:{}}, {start: {name:"Apache flink"}, end: {name:"stream processing"}, properties:{}}, {start: {name:"Apache storm"}, end: {name:"stream processing"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:SUPPORTS]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache samza"}, end: {name:"stream processing"}, properties:{}}, {start: {name:"Apache samza"}, end: {name:"batch processing"}, properties:{}}, {start: {name:"Apache druid"}, end: {name:"OLAP"}, properties:{}}, {start: {name:"Apache beam"}, end: {name:"batch processing"}, properties:{}}, {start: {name:"Apache beam"}, end: {name:"stream processing"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:SUPPORTS]->(end) SET r += row.properties;
UNWIND [{start: {name:"compositional parallel processing engine"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"declarative parallel processing engine"}, end: {name:"parallel processing engine"}, properties:{}}] AS row
MATCH (start:concept{name: row.start.name})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:IS_A]->(end) SET r += row.properties;
UNWIND [{start: {name:"hive"}, end: {_id:38}, properties:{}}, {start: {name:"Apache spark"}, end: {_id:38}, properties:{}}, {start: {name:"Apache impala"}, end: {_id:39}, properties:{}}, {start: {name:"Apache flink"}, end: {_id:38}, properties:{}}, {start: {name:"Apache iceberg"}, end: {_id:38}, properties:{}}, {start: {name:"presto"}, end: {_id:37}, properties:{}}, {start: {name:"delta lake"}, end: {_id:19}, properties:{}}, {start: {name:"Apache drill"}, end: {_id:38}, properties:{}}, {start: {name:"Apache hudi"}, end: {_id:38}, properties:{}}, {start: {name:"Apache storm"}, end: {_id:38}, properties:{}}, {start: {name:"Apache nessie"}, end: {_id:38}, properties:{}}, {start: {name:"Apache arrow flight"}, end: {_id:38}, properties:{}}, {start: {name:"Apache samza"}, end: {_id:38}, properties:{}}, {start: {name:"Apache kafka"}, end: {_id:84}, properties:{}}, {start: {name:"Apache druid"}, end: {_id:38}, properties:{}}, {start: {name:"Apache impala"}, end: {_id:38}, properties:{}}, {start: {name:"Apache hawq"}, end: {_id:38}, properties:{}}, {start: {name:"Apache hbase"}, end: {_id:38}, properties:{}}, {start: {name:"Apache beam"}, end: {_id:38}, properties:{}}, {start: {name:"Apache nemo"}, end: {_id:38}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:FROM]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache airflow"}, end: {_id:38}, properties:{}}, {start: {name:"Apache airflow"}, end: {_id:96}, properties:{}}, {start: {name:"Elastic search"}, end: {_id:97}, properties:{}}, {start: {name:"Apache solr"}, end: {_id:38}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:FROM]->(end) SET r += row.properties;
UNWIND [{start: {_id:65}, end: {_id:67}, properties:{}}] AS row
MATCH (start:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.start._id})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:SIMILAR_TO]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache arrow flight"}, end: {_id:95}, properties:{}}, {start: {name:"Apache arrow flight"}, end: {_id:64}, properties:{}}, {start: {name:"Elastic search"}, end: {_id:12}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:USES]->(end) SET r += row.properties;
UNWIND [{start: {name:"dremio"}, end: {name:"Data mesh"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:ENABLES]->(end) SET r += row.properties;
UNWIND [{start: {name:"Aura DB"}, end: {name:"cypher"}, properties:{}}, {start: {name:"hive"}, end: {name:"hadoop"}, properties:{}}, {start: {name:"Apache impala"}, end: {name:"hadoop"}, properties:{}}, {start: {name:"trino"}, end: {name:"Apache cassandra"}, properties:{}}, {start: {name:"dremio arctic"}, end: {name:"Apache iceberg"}, properties:{}}, {start: {name:"Apache pig"}, end: {name:"hadoop"}, properties:{}}, {start: {name:"delta lake"}, end: {name:"Apache spark"}, properties:{}}, {start: {name:"Apache samza"}, end: {name:"Apache kafka"}, properties:{}}, {start: {name:"Apache samza"}, end: {name:"yarn"}, properties:{}}, {start: {name:"hadoop"}, end: {name:"yarn"}, properties:{}}, {start: {name:"Apache spark Structured Streaming"}, end: {name:"Apache spark SQL"}, properties:{}}, {start: {name:"Apache hawq"}, end: {name:"hadoop"}, properties:{}}, {start: {name:"Apache hbase"}, end: {name:"hadoop"}, properties:{}}, {start: {name:"Apache solr"}, end: {name:"Apache lucene"}, properties:{}}, {start: {name:"Elastic search"}, end: {name:"Apache lucene"}, properties:{}}, {start: {name:"jq-assistant"}, end: {name:"neo4j"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:tool{name: row.end.name})
CREATE (start)-[r:USES]->(end) SET r += row.properties;
UNWIND [{start: {_id:64}, end: {name:"column storage"}, properties:{}}, {start: {_id:65}, end: {name:"column storage"}, properties:{}}, {start: {_id:67}, end: {name:"column storage"}, properties:{}}, {start: {_id:64}, end: {name:"memory layout"}, properties:{}}] AS row
MATCH (start:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.start._id})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:IS_A]->(end) SET r += row.properties;
UNWIND [{start: {_id:65}, end: {name:"Apache arrow flight"}, properties:{}}] AS row
MATCH (start:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.start._id})
MATCH (end:tool{name: row.end.name})
CREATE (start)-[r:SUPPORTS]->(end) SET r += row.properties;
UNWIND [{start: {name:"hadoop"}, end: {_id:47}, properties:{}}, {start: {name:"trino"}, end: {_id:47}, properties:{}}, {start: {name:"Apache hawq"}, end: {_id:47}, properties:{}}, {start: {name:"Apache hbase"}, end: {_id:47}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:USES]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache arrow flight"}, end: {name:"Apache arrow flight sql"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"Apache spark SQL"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"Apache spark Structured Streaming"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"Apache spark GraphX"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:tool{name: row.end.name})
CREATE (start)-[r:PROVIDES]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache apex"}, end: {name:"compositional parallel processing engine"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:IS_A]->(end) SET r += row.properties;
UNWIND [{start: {name:"trino"}, end: {name:"hive"}, properties:{}}, {start: {name:"trino"}, end: {name:"Apache pig"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:tool{name: row.end.name})
CREATE (start)-[r:SIMILAR_TO]->(end) SET r += row.properties;
UNWIND [{start: {name:"prestoSQL"}, end: {name:"trino"}, properties:{}}, {start: {name:"prestoDB"}, end: {name:"presto"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:tool{name: row.end.name})
CREATE (start)-[r:SUCCESSOR]->(end) SET r += row.properties;
UNWIND [{start: {name:"Aura DS"}, end: {name:"data science framework"}, properties:{}}, {start: {name:"Aura DB"}, end: {name:"graph database"}, properties:{}}, {start: {name:"cypher"}, end: {name:"query language"}, properties:{}}, {start: {name:"Apache iceberg"}, end: {name:"table format"}, properties:{}}, {start: {name:"dremio"}, end: {name:"lakehouse"}, properties:{}}, {start: {name:"hive"}, end: {name:"warehouse"}, properties:{}}, {start: {name:"Apache impala"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"trino"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"Apache arrow flight"}, end: {name:"in-memory data structure"}, properties:{}}, {start: {name:"Apache cassandra"}, end: {name:"row storage"}, properties:{}}, {start: {name:"Apache flink"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"Apache cassandra"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"Apache cassandra"}, end: {name:"database"}, properties:{}}, {start: {name:"presto"}, end: {name:"distributed SQL query engine"}, properties:{}}, {start: {name:"dremio sonar"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"hive"}, end: {name:"distributed SQL query engine"}, properties:{}}, {start: {name:"Apache pig"}, end: {name:"distributed SQL query engine"}, properties:{}}, {start: {name:"Apache hudi"}, end: {name:"data lake"}, properties:{}}, {start: {name:"delta lake"}, end: {name:"lakehouse"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:IS_A]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache storm"}, end: {name:"compositional parallel processing engine"}, properties:{}}, {start: {name:"Apache nessie"}, end: {name:"git for data"}, properties:{}}, {start: {name:"snowflake"}, end: {name:"warehouse"}, properties:{}}, {start: {name:"Apache samza"}, end: {name:"compositional parallel processing engine"}, properties:{}}, {start: {name:"Apache flink"}, end: {name:"declarative parallel processing engine"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"declarative parallel processing engine"}, properties:{}}, {start: {name:"Quine"}, end: {name:"graph database"}, properties:{}}, {start: {name:"Quine"}, end: {name:"streaming graph interpreter"}, properties:{}}, {start: {name:"Apache kafka"}, end: {name:"stream processing engine"}, properties:{}}, {start: {name:"Apache drill"}, end: {name:"distributed SQL query engine"}, properties:{}}, {start: {name:"Apache spark Structured Streaming"}, end: {name:"stream processing engine"}, properties:{}}, {start: {name:"Quine"}, end: {name:"stream processing engine"}, properties:{}}, {start: {name:"Apache flink"}, end: {name:"stream processing engine"}, properties:{}}, {start: {name:"teradata Vantage"}, end: {name:"warehouse"}, properties:{}}, {start: {name:"Apache druid"}, end: {name:"column storage"}, properties:{}}, {start: {name:"Apache druid"}, end: {name:"warehouse"}, properties:{}}, {start: {name:"teradata Vantage"}, end: {name:"database"}, properties:{}}, {start: {name:"teradata Vantage"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"Apache hawq"}, end: {name:"parallel processing engine"}, properties:{}}, {start: {name:"Apache hbase"}, end: {name:"database"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:IS_A]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache hbase"}, end: {name:"distributed SQL query engine"}, properties:{}}, {start: {name:"Elastic search"}, end: {name:"search engine"}, properties:{}}, {start: {name:"Apache solr"}, end: {name:"search engine"}, properties:{}}, {start: {name:"graphana"}, end: {name:"visualization engine"}, properties:{}}, {start: {name:"pandas"}, end: {name:"library"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:concept{name: row.end.name})
CREATE (start)-[r:IS_A]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache spark"}, end: {_id:21}, properties:{}}, {start: {name:"Apache hudi"}, end: {_id:65}, properties:{}}, {start: {name:"delta lake"}, end: {_id:65}, properties:{}}, {start: {name:"Apache iceberg"}, end: {_id:65}, properties:{}}, {start: {name:"Apache iceberg"}, end: {_id:76}, properties:{}}, {start: {name:"Apache iceberg"}, end: {_id:75}, properties:{}}, {start: {name:"delta lake"}, end: {_id:68}, properties:{}}, {start: {name:"Apache nessie"}, end: {_id:68}, properties:{}}, {start: {name:"Apache kafka"}, end: {_id:75}, properties:{}}, {start: {name:"Apache kafka"}, end: {_id:12}, properties:{}}, {start: {name:"hive"}, end: {_id:21}, properties:{}}, {start: {name:"hadoop"}, end: {_id:21}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:SUPPORTS]->(end) SET r += row.properties;
UNWIND [{start: {name:"trino"}, end: {name:"Apache iceberg"}, properties:{}}, {start: {name:"Apache impala"}, end: {name:"Apache iceberg"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"Apache iceberg"}, properties:{}}, {start: {name:"Apache flink"}, end: {name:"Apache iceberg"}, properties:{}}, {start: {name:"hive"}, end: {name:"Apache iceberg"}, properties:{}}, {start: {name:"dremio arctic"}, end: {name:"Apache spark"}, properties:{}}, {start: {name:"dremio arctic"}, end: {name:"Apache flink"}, properties:{}}, {start: {name:"dremio arctic"}, end: {name:"trino"}, properties:{}}, {start: {name:"dremio arctic"}, end: {name:"dremio sonar"}, properties:{}}, {start: {name:"Tableau"}, end: {name:"dremio sonar"}, properties:{}}, {start: {name:"Power BI"}, end: {name:"dremio sonar"}, properties:{}}, {start: {name:"dremio sonar"}, end: {name:"Apache arrow flight"}, properties:{}}, {start: {name:"Apache nessie"}, end: {name:"Apache iceberg"}, properties:{}}, {start: {name:"Apache beam"}, end: {name:"Apache flink"}, properties:{}}, {start: {name:"Apache beam"}, end: {name:"Apache spark"}, properties:{}}, {start: {name:"Apache beam"}, end: {name:"Apache samza"}, properties:{}}, {start: {name:"Apache beam"}, end: {name:"Apache nemo"}, properties:{}}, {start: {name:"Apache cassandra"}, end: {name:"Apache arrow flight"}, properties:{}}, {start: {name:"Apache spark"}, end: {name:"Apache arrow flight"}, properties:{}}, {start: {name:"Apache impala"}, end: {name:"Apache arrow flight"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:tool{name: row.end.name})
CREATE (start)-[r:SUPPORTS]->(end) SET r += row.properties;
UNWIND [{start: {name:"Apache hbase"}, end: {name:"Apache arrow flight"}, properties:{}}, {start: {name:"pandas"}, end: {name:"Apache arrow flight"}, properties:{}}] AS row
MATCH (start:tool{name: row.start.name})
MATCH (end:tool{name: row.end.name})
CREATE (start)-[r:SUPPORTS]->(end) SET r += row.properties;
UNWIND [{start: {_id:64}, end: {_id:67}, properties:{}}, {start: {_id:64}, end: {_id:65}, properties:{}}] AS row
MATCH (start:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.start._id})
MATCH (end:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.end._id})
CREATE (start)-[r:USES]->(end) SET r += row.properties;
:commit
:begin
MATCH (n:`UNIQUE IMPORT LABEL`)  WITH n LIMIT 20000 REMOVE n:`UNIQUE IMPORT LABEL` REMOVE n.`UNIQUE IMPORT ID`;
:commit
:begin
DROP CONSTRAINT ON (node:`UNIQUE IMPORT LABEL`) ASSERT (node.`UNIQUE IMPORT ID`) IS UNIQUE;
:commit
